"""
å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆæ™ºèƒ½ä½“
åŸºäºLangChainå’ŒOllamaæ„å»ºçš„æ™ºèƒ½æ–‡æ¡ˆç”Ÿæˆå·¥å…·
"""

import json
import sys
import os
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥LLMæ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from LLM.ollama_client import OllamaClient
from .i18n_agent import (
    Language, 
    get_prompt_template, 
    format_keywords_section, 
    format_special_requirements_section
)

from langchain.agents import initialize_agent, AgentType
from langchain.tools import Tool
from langchain.memory import ConversationBufferMemory
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from pydantic import Field, ConfigDict


class ContentCategory(Enum):
    """å†…å®¹åˆ†ç±»æšä¸¾"""
    BEAUTY = "ç¾å¦†æŠ¤è‚¤"
    FASHION = "æ—¶å°šç©¿æ­"
    FOOD = "ç¾é£Ÿæ¢åº—"
    TRAVEL = "æ—…è¡Œæ”»ç•¥"
    LIFESTYLE = "ç”Ÿæ´»æ–¹å¼"
    FITNESS = "å¥èº«è¿åŠ¨"
    HOME = "å®¶å±…è£…é¥°"
    STUDY = "å­¦ä¹ åˆ†äº«"
    WORK = "èŒåœºå¹²è´§"
    SHOPPING = "å¥½ç‰©æ¨è"


@dataclass
class ContentRequest:
    """å†…å®¹ç”Ÿæˆè¯·æ±‚"""
    category: ContentCategory
    topic: str
    tone: str = "æ´»æ³¼å¯çˆ±"
    length: str = "ä¸­ç­‰"
    keywords: List[str] = None
    target_audience: str = "å¹´è½»å¥³æ€§"
    special_requirements: str = ""
    language: str = "zh-CN"  # æ–°å¢è¯­è¨€å‚æ•°


class OllamaLangChainLLM(LLM):
    """å°†Ollamaå®¢æˆ·ç«¯é€‚é…ä¸ºLangChainçš„LLMæ¥å£"""
    
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    ollama_client: OllamaClient = Field(default_factory=lambda: OllamaClient())
    enable_stream: bool = Field(default=True)
    enable_thinking: bool = Field(default=True)
    
    @property
    def _llm_type(self) -> str:
        return "ollama"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """è°ƒç”¨Ollamaç”Ÿæˆæ–‡æœ¬"""
        # å¤„ç†æ€è€ƒæ¨¡å¼
        if not self.enable_thinking and not prompt.endswith("/no_think"):
            prompt += "/no_think"
        
        response = self.ollama_client.generate(prompt, stream=self.enable_stream)
        return response if response else "æŠ±æ­‰ï¼Œç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚"


class XiaohongshuAgent:
    """å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆæ™ºèƒ½ä½“"""
    
    def __init__(self, enable_stream: bool = True, enable_thinking: bool = True):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        Args:
            enable_stream: æ˜¯å¦å¯ç”¨æµå¼å“åº”
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
        """
        self.ollama_client = OllamaClient()
        self.llm = OllamaLangChainLLM(
            enable_stream=enable_stream,
            enable_thinking=enable_thinking
        )
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # å­˜å‚¨é…ç½®
        self.enable_stream = enable_stream
        self.enable_thinking = enable_thinking
        
        # åˆå§‹åŒ–å·¥å…·
        self.tools = self._create_tools()
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self.agent = initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
            memory=self.memory,
            verbose=True,
            handle_parsing_errors=True
        )
    
    def _create_tools(self) -> List[Tool]:
        """åˆ›å»ºæ™ºèƒ½ä½“ä½¿ç”¨çš„å·¥å…·"""
        
        def generate_title_tool(query: str) -> str:
            """ç”Ÿæˆå°çº¢ä¹¦æ ‡é¢˜çš„å·¥å…·"""
            # ä»æŸ¥è¯¢ä¸­æå–è¯­è¨€ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤è¯­è¨€
            language = Language.ZH_CN
            if "|language:" in query:
                query_parts = query.split("|language:")
                if len(query_parts) > 1:
                    lang_code = query_parts[1].strip()
                    try:
                        language = Language(lang_code)
                        query = query_parts[0].strip()
                    except ValueError:
                        pass
            
            prompt_template = get_prompt_template("title_generation", language)
            prompt = prompt_template.format(query=query)
            
            # å¤„ç†æ€è€ƒæ¨¡å¼
            if not self.enable_thinking:
                prompt += "/no_think"
            
            return self.ollama_client.generate(prompt, stream=self.enable_stream) or "ç”Ÿæˆå¤±è´¥"
        
        def generate_content_tool(query: str) -> str:
            """ç”Ÿæˆå°çº¢ä¹¦æ­£æ–‡å†…å®¹çš„å·¥å…·"""
            # ä»æŸ¥è¯¢ä¸­æå–è¯­è¨€ä¿¡æ¯
            language = Language.ZH_CN
            if "|language:" in query:
                query_parts = query.split("|language:")
                if len(query_parts) > 1:
                    lang_code = query_parts[1].strip()
                    try:
                        language = Language(lang_code)
                        query = query_parts[0].strip()
                    except ValueError:
                        pass
            
            prompt_template = get_prompt_template("content_writing", language)
            prompt = prompt_template.format(query=query)
            
            # å¤„ç†æ€è€ƒæ¨¡å¼
            if not self.enable_thinking:
                prompt += "/no_think"
            
            return self.ollama_client.generate(prompt, stream=self.enable_stream) or "ç”Ÿæˆå¤±è´¥"
        
        def generate_hashtags_tool(query: str) -> str:
            """ç”Ÿæˆå°çº¢ä¹¦è¯é¢˜æ ‡ç­¾çš„å·¥å…·"""
            # ä»æŸ¥è¯¢ä¸­æå–è¯­è¨€ä¿¡æ¯
            language = Language.ZH_CN
            if "|language:" in query:
                query_parts = query.split("|language:")
                if len(query_parts) > 1:
                    lang_code = query_parts[1].strip()
                    try:
                        language = Language(lang_code)
                        query = query_parts[0].strip()
                    except ValueError:
                        pass
            
            prompt_template = get_prompt_template("hashtag_generation", language)
            prompt = prompt_template.format(query=query)
            
            # å¤„ç†æ€è€ƒæ¨¡å¼
            if not self.enable_thinking:
                prompt += "/no_think"
            
            return self.ollama_client.generate(prompt, stream=self.enable_stream) or "ç”Ÿæˆå¤±è´¥"
        
        def content_optimization_tool(query: str) -> str:
            """å†…å®¹ä¼˜åŒ–å»ºè®®å·¥å…·"""
            # ä»æŸ¥è¯¢ä¸­æå–è¯­è¨€ä¿¡æ¯
            language = Language.ZH_CN
            if "|language:" in query:
                query_parts = query.split("|language:")
                if len(query_parts) > 1:
                    lang_code = query_parts[1].strip()
                    try:
                        language = Language(lang_code)
                        query = query_parts[0].strip()
                    except ValueError:
                        pass
            
            prompt_template = get_prompt_template("content_optimization", language)
            prompt = prompt_template.format(content=query)
            
            # å¤„ç†æ€è€ƒæ¨¡å¼
            if not self.enable_thinking:
                prompt += "/no_think"
            
            return self.ollama_client.generate(prompt, stream=self.enable_stream) or "ç”Ÿæˆå¤±è´¥"
        
        return [
            Tool(
                name="ç”Ÿæˆæ ‡é¢˜",
                func=generate_title_tool,
                description="ä¸ºå°çº¢ä¹¦å†…å®¹ç”Ÿæˆå¸å¼•äººçš„æ ‡é¢˜ï¼Œè¾“å…¥å†…å®¹æè¿°å³å¯"
            ),
            Tool(
                name="ç”Ÿæˆæ­£æ–‡",
                func=generate_content_tool,
                description="æ ¹æ®éœ€æ±‚ç”Ÿæˆå®Œæ•´çš„å°çº¢ä¹¦æ–‡æ¡ˆæ­£æ–‡å†…å®¹"
            ),
            Tool(
                name="ç”Ÿæˆè¯é¢˜æ ‡ç­¾",
                func=generate_hashtags_tool,
                description="ä¸ºå°çº¢ä¹¦å†…å®¹ç”Ÿæˆç›¸å…³çš„è¯é¢˜æ ‡ç­¾"
            ),
            Tool(
                name="å†…å®¹ä¼˜åŒ–",
                func=content_optimization_tool,
                description="å¯¹å·²æœ‰çš„å°çº¢ä¹¦æ–‡æ¡ˆæä¾›ä¼˜åŒ–å»ºè®®å’Œæ”¹è¿›æ–¹æ¡ˆ"
            )
        ]
    
    def check_setup(self) -> bool:
        """æ£€æŸ¥æ™ºèƒ½ä½“è®¾ç½®çŠ¶æ€"""
        print("ğŸ” æ­£åœ¨æ£€æŸ¥æ™ºèƒ½ä½“è®¾ç½®...")
        
        # æ£€æŸ¥Ollamaè¿æ¥
        if not self.ollama_client.check_connection():
            print("âŒ OllamaæœåŠ¡è¿æ¥å¤±è´¥")
            return False
        print("âœ… OllamaæœåŠ¡è¿æ¥æ­£å¸¸")
        
        # æ£€æŸ¥æ¨¡å‹
        if not self.ollama_client.check_model_exists():
            print("âš ï¸  æ¨¡å‹ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è½½...")
            if not self.ollama_client.pull_model():
                print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
                return False
        print("âœ… æ¨¡å‹å‡†å¤‡å°±ç»ª")
        
        return True
    
    def generate_complete_post(self, request: ContentRequest) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„å°çº¢ä¹¦æ–‡æ¡ˆ"""
        
        try:
            # è·å–è¯­è¨€å‚æ•°
            language = Language(request.language) if hasattr(request, 'language') and request.language else Language.ZH_CN
        except ValueError:
            language = Language.ZH_CN
        
        # ä½¿ç”¨å›½é™…åŒ–æ¨¡æ¿æ„å»ºéœ€æ±‚æè¿°
        prompt_template = get_prompt_template("content_generation", language)
        
        # æ ¼å¼åŒ–å…³é”®è¯å’Œç‰¹æ®Šè¦æ±‚
        keywords_section = format_keywords_section(request.keywords, language)
        special_requirements_section = format_special_requirements_section(request.special_requirements, language)
        
        requirement = prompt_template.format(
            category=request.category.value,
            topic=request.topic,
            tone=request.tone,
            length=request.length,
            target_audience=request.target_audience,
            keywords_section=keywords_section,
            special_requirements_section=special_requirements_section
        )
        
        try:
            # ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆå†…å®¹
            result = self.agent.run(requirement)
            
            return {
                "success": True,
                "content": result,
                "request": request.__dict__
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "request": request.__dict__
            }
    
    def optimize_content(self, content: str, language: str = "zh-CN") -> Dict[str, Any]:
        """ä¼˜åŒ–ç°æœ‰å†…å®¹"""
        try:
            # è·å–è¯­è¨€å‚æ•°
            try:
                lang = Language(language)
            except ValueError:
                lang = Language.ZH_CN
            
            # ä½¿ç”¨å›½é™…åŒ–æ¨¡æ¿
            prompt_template = get_prompt_template("content_optimization", lang)
            optimization_query = prompt_template.format(content=content)
            
            result = self.agent.run(optimization_query)
            
            return {
                "success": True,
                "original": content,
                "optimized": result
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "original": content
            }
    
    def chat(self, message: str, language: str = "zh-CN") -> str:
        """ä¸æ™ºèƒ½ä½“å¯¹è¯"""
        try:
            # ä¸ºèŠå¤©æ¶ˆæ¯æ·»åŠ è¯­è¨€ä¸Šä¸‹æ–‡
            try:
                lang = Language(language)
            except ValueError:
                lang = Language.ZH_CN
            
            # æ ¹æ®è¯­è¨€æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡
            if lang == Language.EN_US:
                contextualized_message = f"Please respond in English. User message: {message}"
            elif lang == Language.ZH_TW:
                contextualized_message = f"è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚ç”¨æˆ¶è¨Šæ¯ï¼š{message}"
            elif lang == Language.JA_JP:
                contextualized_message = f"æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š{message}"
            else:
                contextualized_message = f"è¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ã€‚ç”¨æˆ·æ¶ˆæ¯ï¼š{message}"
            
            response = self.agent.run(contextualized_message)
            return response
        except Exception as e:
            # æ ¹æ®è¯­è¨€è¿”å›é”™è¯¯æ¶ˆæ¯
            error_messages = {
                Language.ZH_CN: f"å¯¹è¯å‡ºé”™ï¼š{str(e)}",
                Language.EN_US: f"Chat error: {str(e)}",
                Language.ZH_TW: f"å°è©±å‡ºéŒ¯ï¼š{str(e)}",
                Language.JA_JP: f"ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ©ãƒ¼ï¼š{str(e)}"
            }
            try:
                lang = Language(language)
                return error_messages.get(lang, error_messages[Language.ZH_CN])
            except ValueError:
                return error_messages[Language.ZH_CN]
    
    def update_config(self, enable_stream: bool = None, enable_thinking: bool = None):
        """æ›´æ–°é…ç½®"""
        if enable_stream is not None:
            self.enable_stream = enable_stream
            self.llm.enable_stream = enable_stream
        
        if enable_thinking is not None:
            self.enable_thinking = enable_thinking
            self.llm.enable_thinking = enable_thinking
    
    def generate_complete_post_stream(self, request: ContentRequest, enable_thinking: bool = None):
        """æµå¼ç”Ÿæˆå®Œæ•´çš„å°çº¢ä¹¦æ–‡æ¡ˆ"""
        
        try:
            # è·å–è¯­è¨€å‚æ•°
            language = Language(request.language) if hasattr(request, 'language') and request.language else Language.ZH_CN
        except ValueError:
            language = Language.ZH_CN
        
        # ä½¿ç”¨å›½é™…åŒ–æ¨¡æ¿æ„å»ºéœ€æ±‚æè¿°
        prompt_template = get_prompt_template("content_generation", language)
        
        # æ ¼å¼åŒ–å…³é”®è¯å’Œç‰¹æ®Šè¦æ±‚
        keywords_section = format_keywords_section(request.keywords, language)
        special_requirements_section = format_special_requirements_section(request.special_requirements, language)
        
        requirement = prompt_template.format(
            category=request.category.value,
            topic=request.topic,
            tone=request.tone,
            length=request.length,
            target_audience=request.target_audience,
            keywords_section=keywords_section,
            special_requirements_section=special_requirements_section
        )
        
        # å¤„ç†æ€è€ƒæ¨¡å¼ - ä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨å®ä¾‹è®¾ç½®
        thinking_enabled = enable_thinking if enable_thinking is not None else self.enable_thinking
        if not thinking_enabled:
            requirement += "/no_think"
        
        # ä½¿ç”¨æµå¼ç”Ÿæˆå™¨
        return self.ollama_client.generate_stream(requirement)

    def chat_stream(self, message: str, language: str = "zh-CN", enable_thinking: bool = None):
        """æµå¼å¯¹è¯"""
        try:
            # ä¸ºèŠå¤©æ¶ˆæ¯æ·»åŠ è¯­è¨€ä¸Šä¸‹æ–‡
            try:
                lang = Language(language)
            except ValueError:
                lang = Language.ZH_CN
            
            # æ ¹æ®è¯­è¨€æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡
            if lang == Language.EN_US:
                contextualized_message = f"Please respond in English. User message: {message}"
            elif lang == Language.ZH_TW:
                contextualized_message = f"è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚ç”¨æˆ¶è¨Šæ¯ï¼š{message}"
            elif lang == Language.JA_JP:
                contextualized_message = f"æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š{message}"
            else:
                contextualized_message = f"è¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ã€‚ç”¨æˆ·æ¶ˆæ¯ï¼š{message}"
            
            # å¤„ç†æ€è€ƒæ¨¡å¼ - ä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨å®ä¾‹è®¾ç½®
            thinking_enabled = enable_thinking if enable_thinking is not None else self.enable_thinking
            if not thinking_enabled and not contextualized_message.endswith("/no_think"):
                contextualized_message += "/no_think"
            
            # æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = [{"role": "user", "content": contextualized_message}]
            
            # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
            chat_history = self.memory.chat_memory.messages
            if chat_history:
                # è½¬æ¢LangChainæ¶ˆæ¯æ ¼å¼åˆ°Ollamaæ ¼å¼
                for msg in chat_history[-10:]:  # åªä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
                    if hasattr(msg, 'content'):
                        if isinstance(msg, HumanMessage):
                            messages.insert(-1, {"role": "user", "content": msg.content})
                        elif isinstance(msg, AIMessage):
                            messages.insert(-1, {"role": "assistant", "content": msg.content})
            
            # ä½¿ç”¨æµå¼ç”Ÿæˆå™¨
            return self.ollama_client.chat_stream(messages)
        except Exception as e:
            def error_generator():
                # æ ¹æ®è¯­è¨€è¿”å›é”™è¯¯æ¶ˆæ¯
                error_messages = {
                    Language.ZH_CN: f"å¯¹è¯å‡ºé”™ï¼š{str(e)}",
                    Language.EN_US: f"Chat error: {str(e)}",
                    Language.ZH_TW: f"å°è©±å‡ºéŒ¯ï¼š{str(e)}",
                    Language.JA_JP: f"ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ©ãƒ¼ï¼š{str(e)}"
                }
                try:
                    lang = Language(language)
                    yield error_messages.get(lang, error_messages[Language.ZH_CN])
                except ValueError:
                    yield error_messages[Language.ZH_CN]
            return error_generator()

    def optimize_content_stream(self, content: str, language: str = "zh-CN", enable_thinking: bool = None):
        """æµå¼ä¼˜åŒ–ç°æœ‰å†…å®¹"""
        try:
            # è·å–è¯­è¨€å‚æ•°
            lang = Language(language)
        except ValueError:
            lang = Language.ZH_CN
        
        # ä½¿ç”¨å›½é™…åŒ–æ¨¡æ¿
        prompt_template = get_prompt_template("content_optimization", lang)
        optimization_query = prompt_template.format(content=content)
        
        # å¤„ç†æ€è€ƒæ¨¡å¼ - ä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨å®ä¾‹è®¾ç½®
        thinking_enabled = enable_thinking if enable_thinking is not None else self.enable_thinking
        if not thinking_enabled:
            optimization_query += "/no_think"
        
        # ä½¿ç”¨æµå¼ç”Ÿæˆå™¨
        return self.ollama_client.generate_stream(optimization_query)

    def intelligent_loop(self, content: str, user_feedback: str, content_request: ContentRequest = None):
        """æ™ºèƒ½ä½“å›ç¯å¤„ç†
        
        Args:
            content: å½“å‰ç”Ÿæˆçš„å†…å®¹
            user_feedback: ç”¨æˆ·åé¦ˆ ("ä¸æ»¡æ„", "æ»¡æ„", "éœ€è¦ä¼˜åŒ–", "é‡æ–°ç”Ÿæˆ")
            content_request: åŸå§‹å†…å®¹è¯·æ±‚ï¼Œç”¨äºé‡æ–°ç”Ÿæˆ
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        try:
            if user_feedback == "ä¸æ»¡æ„" or user_feedback == "é‡æ–°ç”Ÿæˆ":
                # ç”¨æˆ·ä¸æ»¡æ„ï¼Œé‡æ–°ç”Ÿæˆå†…å®¹
                if content_request:
                    return self.regenerate_with_improvements(content_request, content)
                else:
                    # å¦‚æœæ²¡æœ‰åŸå§‹è¯·æ±‚ï¼Œå°è¯•ä»å†…å®¹ä¸­æ¨æ–­å¹¶é‡æ–°ç”Ÿæˆ
                    return self.regenerate_from_content(content)
                    
            elif user_feedback == "æ»¡æ„":
                # ç”¨æˆ·æ»¡æ„ï¼Œè¯¢é—®æ˜¯å¦éœ€è¦ä¼˜åŒ–
                return {
                    "success": True,
                    "action": "ask_optimization",
                    "message": "å¾ˆé«˜å…´æ‚¨æ»¡æ„è¿™ä¸ªæ–‡æ¡ˆï¼æ˜¯å¦éœ€è¦æˆ‘è¿›ä¸€æ­¥ä¼˜åŒ–å†…å®¹ï¼Ÿ",
                    "options": ["éœ€è¦ä¼˜åŒ–", "ä¸éœ€è¦ä¼˜åŒ–ï¼Œå·²å®Œæˆ"]
                }
                
            elif user_feedback == "éœ€è¦ä¼˜åŒ–":
                # ç”¨æˆ·éœ€è¦ä¼˜åŒ–ï¼Œæ‰§è¡Œæ™ºèƒ½ä¼˜åŒ–
                return self.optimize_content(content)
                
            elif user_feedback == "ä¸éœ€è¦ä¼˜åŒ–ï¼Œå·²å®Œæˆ":
                # ç”¨æˆ·å®Œå…¨æ»¡æ„ï¼Œç»“æŸæµç¨‹
                return {
                    "success": True,
                    "action": "completed",
                    "message": "åˆ›ä½œå®Œæˆï¼å¦‚éœ€è¦æ–°çš„æ–‡æ¡ˆï¼Œè¯·å¼€å§‹æ–°çš„åˆ›ä½œæµç¨‹ã€‚",
                    "final_content": content
                }
                
            else:
                # æœªçŸ¥åé¦ˆï¼Œæä¾›å¸®åŠ©
                return {
                    "success": False,
                    "error": "æœªè¯†åˆ«çš„åé¦ˆç±»å‹",
                    "message": "è¯·é€‰æ‹©ï¼šä¸æ»¡æ„ã€æ»¡æ„ã€éœ€è¦ä¼˜åŒ– æˆ– ä¸éœ€è¦ä¼˜åŒ–ï¼Œå·²å®Œæˆ"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·é‡è¯•"
            }
    
    def regenerate_with_improvements(self, request: ContentRequest, previous_content: str):
        """åŸºäºç”¨æˆ·ä¸æ»¡æ„é‡æ–°ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬"""
        try:
            # è·å–è¯­è¨€å‚æ•°
            try:
                language = Language(request.language) if hasattr(request, 'language') and request.language else Language.ZH_CN
            except ValueError:
                language = Language.ZH_CN
            
            # ä½¿ç”¨å›½é™…åŒ–æ¨¡æ¿
            prompt_template = get_prompt_template("regeneration_with_improvements", language)
            
            # æ ¼å¼åŒ–å…³é”®è¯å’Œç‰¹æ®Šè¦æ±‚
            keywords_section = format_keywords_section(request.keywords, language)
            special_requirements_section = format_special_requirements_section(request.special_requirements, language)
            
            improvement_prompt = prompt_template.format(
                category=request.category.value,
                topic=request.topic,
                tone=request.tone,
                length=request.length,
                target_audience=request.target_audience,
                keywords_section=keywords_section,
                special_requirements_section=special_requirements_section,
                previous_content=previous_content
            )
            
            # å¤„ç†æ€è€ƒæ¨¡å¼
            if not self.enable_thinking:
                improvement_prompt += "/no_think"
            
            result = self.ollama_client.generate(improvement_prompt, stream=self.enable_stream)
            
            return {
                "success": True,
                "action": "regenerated",
                "content": result,
                "message": "å·²é‡æ–°ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬ï¼Œè¯·æŸ¥çœ‹æ˜¯å¦æ»¡æ„"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "action": "error"
            }
    
    def regenerate_from_content(self, content: str, language: str = "zh-CN"):
        """ä»ç°æœ‰å†…å®¹æ¨æ–­éœ€æ±‚å¹¶é‡æ–°ç”Ÿæˆ"""
        try:
            # è·å–è¯­è¨€å‚æ•°
            try:
                lang = Language(language)
            except ValueError:
                lang = Language.ZH_CN
            
            # ä½¿ç”¨å›½é™…åŒ–æ¨¡æ¿
            prompt_template = get_prompt_template("regeneration_from_content", lang)
            regeneration_prompt = prompt_template.format(content=content)
            
            # å¤„ç†æ€è€ƒæ¨¡å¼
            if not self.enable_thinking:
                regeneration_prompt += "/no_think"
            
            result = self.ollama_client.generate(regeneration_prompt, stream=self.enable_stream)
            
            return {
                "success": True,
                "action": "regenerated", 
                "content": result,
                "message": "å·²åŸºäºåŸå†…å®¹é‡æ–°ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "action": "error"
            }
    
    def intelligent_loop_stream(self, content: str, user_feedback: str, content_request: ContentRequest = None):
        """æµå¼æ™ºèƒ½ä½“å›ç¯å¤„ç†"""
        try:
            if user_feedback == "ä¸æ»¡æ„" or user_feedback == "é‡æ–°ç”Ÿæˆ":
                # é‡æ–°ç”Ÿæˆæµå¼ç‰ˆæœ¬
                if content_request:
                    return self.regenerate_with_improvements_stream(content_request, content)
                else:
                    return self.regenerate_from_content_stream(content)
                    
            elif user_feedback == "éœ€è¦ä¼˜åŒ–":
                # æµå¼ä¼˜åŒ–
                return self.optimize_content_stream(content)
                
            else:
                # å¯¹äºå…¶ä»–æƒ…å†µï¼Œè¿”å›ç®€å•çš„ç”Ÿæˆå™¨
                def simple_response():
                    if user_feedback == "æ»¡æ„":
                        yield "å¾ˆé«˜å…´æ‚¨æ»¡æ„è¿™ä¸ªæ–‡æ¡ˆï¼æ˜¯å¦éœ€è¦æˆ‘è¿›ä¸€æ­¥ä¼˜åŒ–å†…å®¹ï¼Ÿ"
                    elif user_feedback == "ä¸éœ€è¦ä¼˜åŒ–ï¼Œå·²å®Œæˆ":
                        yield "åˆ›ä½œå®Œæˆï¼å¦‚éœ€è¦æ–°çš„æ–‡æ¡ˆï¼Œè¯·å¼€å§‹æ–°çš„åˆ›ä½œæµç¨‹ã€‚"
                    else:
                        yield "è¯·é€‰æ‹©ï¼šä¸æ»¡æ„ã€æ»¡æ„ã€éœ€è¦ä¼˜åŒ– æˆ– ä¸éœ€è¦ä¼˜åŒ–ï¼Œå·²å®Œæˆ"
                
                return simple_response()
                
        except Exception as e:
            def error_response():
                yield f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            return error_response()
    
    def regenerate_with_improvements_stream(self, request: ContentRequest, previous_content: str):
        """æµå¼é‡æ–°ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬"""
        # è·å–è¯­è¨€å‚æ•°
        try:
            language = Language(request.language) if hasattr(request, 'language') and request.language else Language.ZH_CN
        except ValueError:
            language = Language.ZH_CN
        
        # ä½¿ç”¨å›½é™…åŒ–æ¨¡æ¿
        prompt_template = get_prompt_template("regeneration_with_improvements", language)
        
        # æ ¼å¼åŒ–å…³é”®è¯å’Œç‰¹æ®Šè¦æ±‚
        keywords_section = format_keywords_section(request.keywords, language)
        special_requirements_section = format_special_requirements_section(request.special_requirements, language)
        
        improvement_prompt = prompt_template.format(
            category=request.category.value,
            topic=request.topic,
            tone=request.tone,
            length=request.length,
            target_audience=request.target_audience,
            keywords_section=keywords_section,
            special_requirements_section=special_requirements_section,
            previous_content=previous_content
        )
        
        # å¤„ç†æ€è€ƒæ¨¡å¼
        if not self.enable_thinking:
            improvement_prompt += "/no_think"
        
        return self.ollama_client.generate_stream(improvement_prompt)
    
    def regenerate_from_content_stream(self, content: str, language: str = "zh-CN"):
        """æµå¼ä»ç°æœ‰å†…å®¹é‡æ–°ç”Ÿæˆ"""
        # è·å–è¯­è¨€å‚æ•°
        try:
            lang = Language(language)
        except ValueError:
            lang = Language.ZH_CN
        
        # ä½¿ç”¨å›½é™…åŒ–æ¨¡æ¿
        prompt_template = get_prompt_template("regeneration_from_content", lang)
        regeneration_prompt = prompt_template.format(content=content)
        
        # å¤„ç†æ€è€ƒæ¨¡å¼
        if not self.enable_thinking:
            regeneration_prompt += "/no_think"
        
        return self.ollama_client.generate_stream(regeneration_prompt)


def main():
    """æ¼”ç¤ºæ™ºèƒ½ä½“ä½¿ç”¨"""
    print("ğŸ‰ å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆæ™ºèƒ½ä½“å¯åŠ¨ä¸­...")
    
    # é…ç½®é€‰æ‹©
    print("ğŸ”§ é…ç½®é€‰é¡¹ï¼š")
    stream_choice = input("æ˜¯å¦å¯ç”¨æµå¼å“åº”ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").lower()
    enable_stream = stream_choice != 'n'
    
    thinking_choice = input("æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").lower()
    enable_thinking = thinking_choice != 'n'
    
    print(f"âœ… é…ç½®ï¼šæµå¼å“åº”={enable_stream}, æ€è€ƒæ¨¡å¼={enable_thinking}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = XiaohongshuAgent(enable_stream=enable_stream, enable_thinking=enable_thinking)
    
    # æ£€æŸ¥è®¾ç½®
    if not agent.check_setup():
        print("âŒ æ™ºèƒ½ä½“è®¾ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡")
        return
    
    print("âœ… æ™ºèƒ½ä½“å‡†å¤‡å°±ç»ªï¼")
    print("=" * 50)
    
    # ç¤ºä¾‹ï¼šç”Ÿæˆç¾å¦†å†…å®¹
    print("ğŸ“ ç¤ºä¾‹ï¼šç”Ÿæˆç¾å¦†æŠ¤è‚¤å†…å®¹")
    request = ContentRequest(
        category=ContentCategory.BEAUTY,
        topic="å†¬å­£æŠ¤è‚¤ä¿æ¹¿æ”»ç•¥",
        tone="ä¸“ä¸šæ¸©å’Œ",
        keywords=["ä¿æ¹¿", "å†¬å­£", "æŠ¤è‚¤"],
        target_audience="20-30å²å¥³æ€§"
    )
    
    result = agent.generate_complete_post(request)
    
    if result["success"]:
        print("ç”Ÿæˆç»“æœï¼š")
        print(result["content"])
    else:
        print(f"ç”Ÿæˆå¤±è´¥ï¼š{result['error']}")
    
    print("=" * 50)
    print("ğŸ’¬ æ‚¨å¯ä»¥ç»§ç»­ä¸æ™ºèƒ½ä½“å¯¹è¯ï¼Œè¾“å…¥'quit'é€€å‡º")
    
    # äº¤äº’æ¨¡å¼
    while True:
        try:
            user_input = input("\næ‚¨ï¼š")
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                break
            
            response = agent.chat(user_input)
            print(f"\næ™ºèƒ½ä½“ï¼š{response}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨å°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆæ™ºèƒ½ä½“ï¼")
            break


if __name__ == "__main__":
    main() 